{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Sure, here's the updated cheatsheet:\n",
    "\n",
    "1. **Creating a sample from normal, binomial, poisson, exponential distributions using numpy:**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Normal distribution sample\n",
    "# loc: Mean (“centre”) of the distribution.\n",
    "# scale: Standard deviation (spread or “width”) of the distribution.\n",
    "# size: Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn.\n",
    "normal_sample = np.random.normal(loc=0, scale=1, size=100)\n",
    "\n",
    "# Binomial distribution sample\n",
    "# n: Parameter of the distribution, >= 0. Floats are also accepted, but they will be truncated to integers.\n",
    "# p: Parameter of the distribution, >= 0 and <=1.\n",
    "# size: Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn.\n",
    "binomial_sample = np.random.binomial(n=10, p=0.5, size=100)\n",
    "\n",
    "# Poisson distribution sample\n",
    "# lam: Expectation of interval, should be >= 0. A sequence of expectation intervals must be broadcastable over the requested size.\n",
    "# size: Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn.\n",
    "poisson_sample = np.random.poisson(lam=5, size=100)\n",
    "\n",
    "# Exponential distribution sample\n",
    "# scale: The scale parameter, > 0.\n",
    "# size: Output shape. If the given shape is, e.g., (m, n, k), then m * n * k samples are drawn.\n",
    "exponential_sample = np.random.exponential(scale=1.0, size=100)\n",
    "```\n",
    "\n",
    "2. **Performing hypothesis tests and interpreting the results:**\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "# One-sample t-test\n",
    "one_sample = stats.ttest_1samp(a= data, popmean= expected_mean)\n",
    "\n",
    "# Two-sample t-test\n",
    "two_sample = stats.ttest_ind(a= data1, b= data2)\n",
    "\n",
    "# One-sample z-test\n",
    "one_sample_z = statsmodels.stats.weightstats.ztest(x1= data, value= expected_mean)\n",
    "\n",
    "# Two-sample z-test\n",
    "two_sample_z = statsmodels.stats.weightstats.ztest(x1= data1, x2= data2)\n",
    "```\n",
    "Use t-tests when the sample size is small (<30) and/or the population standard deviation is unknown. Use z-tests when the sample size is large and the population standard deviation is known.\n",
    "\n",
    "In hypothesis testing, we start with a null hypothesis (H0) and an alternative hypothesis (H1 or Ha). If the p-value is less than or equal to the significance level (usually 0.05), we reject the null hypothesis. If the p-value is greater than the significance level, we fail to reject the null hypothesis. A smaller p-value means that there is stronger evidence in favor of the alternative hypothesis.\n",
    "\n",
    "3. **Shortcut for common p-values for normal distribution and t-distribution:**\n",
    "\n",
    "```python\n",
    "# Normal distribution\n",
    "p_value_norm = stats.norm.sf(x= z_score) #one-sided\n",
    "\n",
    "# T-distribution\n",
    "p_value_t = stats.t.sf(x= t_score, df= degrees_of_freedom) #one-sided\n",
    "```\n",
    "\n",
    "4. **Using np.random.choice:**\n",
    "\n",
    "```python\n",
    "# np.random.choice generates a random sample from a given 1-D array\n",
    "sample = np.random.choice(a= [0, 1], size= 100, replace= True, p= [0.5, 0.5])\n",
    "```\n",
    "Use np.random.choice when you want to generate a random sample from a given 1-D array.\n",
    "\n",
    "5. **Using df.sample():**\n",
    "\n",
    "```python\n",
    "# df.sample() returns a random sample of items from an axis of object.\n",
    "sample = df.sample(n=10)\n",
    "```\n",
    "Use df.sample() when you want to get a random sample of items from your DataFrame.\n",
    "\n",
    "6. **Type 1 and Type 2 errors:**\n",
    "   - Type 1 error (False Positive): Rejecting the null hypothesis when it is true.\n",
    "   - Type 2 error (False Negative): Failing to reject the null hypothesis when it is false.\n",
    "\n",
    "7. **Using .cdf() and .pdf():**\n",
    "\n",
    "```python\n",
    "# .cdf() gives the cumulative distribution function\n",
    "cdf_value = stats.norm.cdf(x= value, loc= mean, scale= std_dev)\n",
    "\n",
    "# .pdf() gives the probability density function\n",
    "pdf_value = stats.norm.pdf(x= value, loc= mean, scale= std_dev)\n",
    "```\n",
    "Use .cdf() when you want to find the probability that a random observation that is taken from the population will be less than or equal to a certain value. Use .pdf() when you want to find the likelihood of the random variable taking a particular value.\n",
    "\n",
    "8. **Bootstrapping distribution:**\n",
    "\n",
    "```python\n",
    "# Bootstrapping is a resampling technique used to estimate statistics on a population by sampling a dataset with replacement.\n",
    "bootstrap_sample = np.random.choice(a= data, size= len(data), replace= True)\n",
    "```\n",
    "Use bootstrapping when you want to understand the uncertainty associated with a given machine learning model or statistical analysis.\n"
   ],
   "id": "c70d408fe61c8bcf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Sure, here's a cheat sheet for the requested topics:\n",
    "\n",
    "**Statistical Distributions:**\n",
    "\n",
    "1. **Binomial Distribution:** A discrete probability distribution of the number of successes in a sequence of n independent experiments. It's characterized by two parameters: the number of trials (n) and the probability of success on each trial (p).\n",
    "\n",
    "```python\n",
    "from scipy.stats import binom\n",
    "# n: number of trials, p: probability of success\n",
    "binom.rvs(n=10, p=0.5)\n",
    "```\n",
    "\n",
    "2. **Normal Distribution:** A continuous probability distribution that is symmetric about the mean, showing that data near the mean are more frequent in occurrence than data far from the mean.\n",
    "\n",
    "```python\n",
    "from scipy.stats import norm\n",
    "# loc: mean, scale: standard deviation\n",
    "norm.rvs(loc=0, scale=1)\n",
    "```\n",
    "\n",
    "3. **Poisson Distribution:** A discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space.\n",
    "\n",
    "```python\n",
    "from scipy.stats import poisson\n",
    "# mu: mean number of events per interval\n",
    "poisson.rvs(mu=5)\n",
    "```\n",
    "\n",
    "4. **T-Distribution:** A type of probability distribution that is symmetric and bell-shaped, like the standard normal distribution, but has heavier tails, meaning it is more prone to producing values that fall far from its mean.\n",
    "\n",
    "```python\n",
    "from scipy.stats import t\n",
    "# df: degrees of freedom\n",
    "t.rvs(df=10)\n",
    "```\n",
    "\n",
    "5. **Chi-Square Distribution:** A distribution of a sum of the squares of k independent standard normal random variables. It's often used in hypothesis testing.\n",
    "\n",
    "```python\n",
    "from scipy.stats import chi2\n",
    "# df: degrees of freedom\n",
    "chi2.rvs(df=2)\n",
    "```\n",
    "\n",
    "6. **F-Distribution:** A distribution that arises in the testing of whether two observed samples have the same variance.\n",
    "\n",
    "```python\n",
    "from scipy.stats import f\n",
    "# dfn: degrees of freedom in numerator, dfd: degrees of freedom in denominator\n",
    "f.rvs(dfn=1, dfd=48)\n",
    "```\n",
    "\n",
    "**Hypothesis Testing:**\n",
    "\n",
    "1. **Null Hypothesis (H0):** The hypothesis that there is no significant difference between specified populations, any observed difference being due to sampling or experimental error.\n",
    "\n",
    "2. **Alternative Hypothesis (H1 or Ha):** The hypothesis that there is a significant difference between specified populations.\n",
    "\n",
    "3. **One-Tailed Hypothesis Test:** A test where the region of rejection is on only one side of the sampling distribution.\n",
    "\n",
    "4. **Two-Tailed Hypothesis Test:** A test where the region of rejection is on both sides of the sampling distribution.\n",
    "\n",
    "**Experimental Design:**\n",
    "\n",
    "1. **Control Group:** A group in an experiment or study that does not receive treatment by the researchers and is then used as a benchmark to measure how the other tested subjects do.\n",
    "\n",
    "2. **Randomization:** The practice of assigning subjects to treatments in such a way that each subject has an equal chance of being assigned to any treatment.\n",
    "\n",
    "3. **Confounding Variables:** Variables that the researcher failed to control, or eliminate, damaging the internal validity of an experiment.\n",
    "\n",
    "**Parameter Estimation and Confidence Intervals:**\n",
    "\n",
    "1. **Parameter Estimation:** The process of using sample data to estimate the parameters of the selected distribution.\n",
    "\n",
    "2. **Confidence Intervals:** A range of values, derived from a statistical estimation process, that is likely to contain the value of an unknown parameter. For example, a 95% confidence interval means that 95% of the time, the parameter will fall within this range.\n",
    "\n",
    "```python\n",
    "from scipy.stats import norm\n",
    "# Calculate a 95% confidence interval\n",
    "confidence_interval = norm.interval(0.95, loc=mean, scale=std_dev)\n",
    "```"
   ],
   "id": "ae641f7ceabfe218"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Sure, here's a cheat sheet for different types of random sampling techniques, how to sample data from a statistical distribution, and how to calculate a probability from a statistical distribution using Python:\n",
    "\n",
    "**Random Sampling Techniques:**\n",
    "\n",
    "1. **Simple Random Sampling:** Every member of the population has an equal chance of being selected. It's implemented in Python using the `random.sample()` function.\n",
    "\n",
    "```python\n",
    "import random\n",
    "\n",
    "population = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "sample = random.sample(population, k=5)\n",
    "```\n",
    "\n",
    "2. **Stratified Sampling:** The population is divided into subgroups (strata) and random samples are taken from each stratum. This can be implemented in Python using the `groupby()` function from pandas and then applying `random.sample()` to each group.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'group': ['A', 'A', 'A', 'B', 'B', 'B', 'C', 'C', 'C'],\n",
    "    'value': [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "})\n",
    "\n",
    "stratified_sample = df.groupby('group').apply(lambda x: x.sample(n=1))\n",
    "```\n",
    "\n",
    "3. **Cluster Sampling:** The population is divided into clusters (groups) and a set of clusters are selected at random. This can be implemented in Python using the `random.sample()` function to select clusters.\n",
    "\n",
    "```python\n",
    "clusters = [['A1', 'A2', 'A3'], ['B1', 'B2', 'B3'], ['C1', 'C2', 'C3']]\n",
    "sample_clusters = random.sample(clusters, k=2)\n",
    "```\n",
    "\n",
    "**Sampling from a Statistical Distribution:**\n",
    "\n",
    "1. **Normal Distribution:**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "normal_sample = np.random.normal(loc=0, scale=1, size=100)\n",
    "```\n",
    "\n",
    "2. **Binomial Distribution:**\n",
    "\n",
    "```python\n",
    "binomial_sample = np.random.binomial(n=10, p=0.5, size=100)\n",
    "```\n",
    "\n",
    "3. **Poisson Distribution:**\n",
    "\n",
    "```python\n",
    "poisson_sample = np.random.poisson(lam=5, size=100)\n",
    "```\n",
    "\n",
    "4. **Exponential Distribution:**\n",
    "\n",
    "```python\n",
    "exponential_sample = np.random.exponential(scale=1.0, size=100)\n",
    "```\n",
    "\n",
    "**Calculating a Probability from a Statistical Distribution:**\n",
    "\n",
    "1. **Normal Distribution:**\n",
    "\n",
    "```python\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Probability that a random variable from a standard normal distribution is less than 1\n",
    "prob = norm.cdf(1, loc=0, scale=1)\n",
    "```\n",
    "\n",
    "2. **Binomial Distribution:**\n",
    "\n",
    "```python\n",
    "from scipy.stats import binom\n",
    "\n",
    "# Probability of getting 5 successes in 10 trials with a success probability of 0.5\n",
    "prob = binom.pmf(k=5, n=10, p=0.5)\n",
    "```\n",
    "\n",
    "3. **Poisson Distribution:**\n",
    "\n",
    "```python\n",
    "from scipy.stats import poisson\n",
    "\n",
    "# Probability of getting exactly 5 events in an interval where the average rate of events is 5\n",
    "prob = poisson.pmf(k=5, mu=5)\n",
    "```\n",
    "\n",
    "4. **Exponential Distribution:**\n",
    "\n",
    "```python\n",
    "from scipy.stats import expon\n",
    "\n",
    "# Probability that a random variable from an exponential distribution with scale parameter 1 is less than 1\n",
    "prob = expon.cdf(1, scale=1)\n",
    "```"
   ],
   "id": "e74a6728be1f8188"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Both `np.random.binomial()` and `scipy.stats.binom.rvs()` are used to generate random numbers from a binomial distribution. However, they belong to different libraries and have slightly different functionalities.\n",
    "\n",
    "`np.random.binomial(n, p, size=None)` is a function from the NumPy library. It generates random numbers from a binomial distribution defined by the number of trials `n` and the probability of success `p`. The `size` parameter determines the shape of the output.\n",
    "\n",
    "`scipy.stats.binom.rvs(n, p, size=1, random_state=None)` is a function from the SciPy library. It also generates random numbers from a binomial distribution defined by `n` and `p`. The `size` parameter determines the shape of the output, and `random_state` parameter can be used to seed the random number generator for reproducibility.\n",
    "\n",
    "In terms of when to use which, it generally depends on your specific needs and which library you are already using in your code. If you're already using NumPy for other tasks, it might be more convenient to use `np.random.binomial()`. If you're using SciPy and need to set a random seed for reproducibility, `scipy.stats.binom.rvs()` would be the better choice. Both functions will give you similar results when used with the same parameters."
   ],
   "id": "332bc8a0364e55d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Sure, here's a cheat sheet for handling outliers in a data sample using Python:\n",
    "\n",
    "**1. Z-Score:**\n",
    "\n",
    "The Z-score is a measure of how many standard deviations an element is from the mean. If the Z-score of a data point is more than 3, it indicates that the data point is quite different from the other data points. Such a data point can be an outlier.\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([1, 2, 2, 2, 3, 1, 1, 15, 2, 2, 2, 3, 1, 1, 2])\n",
    "z_scores = stats.zscore(data)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 3)\n",
    "new_data = data[filtered_entries]\n",
    "```\n",
    "\n",
    "**2. IQR Method:**\n",
    "\n",
    "The Interquartile Range (IQR) method uses the IQR to detect outliers. The IQR is the range between the first quartile (25th percentile) and the third quartile (75th percentile). Any data points that fall below Q1 - 1.5 IQR or above Q3 + 1.5 IQR are considered outliers.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([1, 2, 2, 2, 3, 1, 1, 15, 2, 2, 2, 3, 1, 1, 2])\n",
    "Q1 = np.percentile(data, 25)\n",
    "Q3 = np.percentile(data, 75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "new_data = data[(data > lower_bound) & (data < upper_bound)]\n",
    "```\n",
    "\n",
    "**3. Removing Outliers using Standard Deviation:**\n",
    "\n",
    "If a data point is more than 3 standard deviations from the mean, it is considered an outlier.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([1, 2, 2, 2, 3, 1, 1, 15, 2, 2, 2, 3, 1, 1, 2])\n",
    "mean = np.mean(data)\n",
    "std_dev = np.std(data)\n",
    "cut_off = std_dev * 3\n",
    "lower_bound, upper_bound = mean - cut_off, mean + cut_off\n",
    "new_data = data[(data > lower_bound) & (data < upper_bound)]\n",
    "```\n",
    "\n",
    "Remember, before removing outliers, it's important to investigate the nature of the outlier. Sometimes, outliers are legitimate data points that indicate important findings or errors in data collection."
   ],
   "id": "151b107057934fef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Sure, here's a cheat sheet for using aggregate functions in Python with the pandas library:\n",
    "\n",
    "**1. Sum:**\n",
    "\n",
    "The `sum()` function returns the sum of the values for the requested axis.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "total = df['A'].sum()\n",
    "```\n",
    "\n",
    "**2. Mean:**\n",
    "\n",
    "The `mean()` function returns the mean of the values for the requested axis.\n",
    "\n",
    "```python\n",
    "average = df['A'].mean()\n",
    "```\n",
    "\n",
    "**3. Min:**\n",
    "\n",
    "The `min()` function returns the minimum of the values for the requested axis.\n",
    "\n",
    "```python\n",
    "minimum = df['A'].min()\n",
    "```\n",
    "\n",
    "**4. Max:**\n",
    "\n",
    "The `max()` function returns the maximum of the values for the requested axis.\n",
    "\n",
    "```python\n",
    "maximum = df['A'].max()\n",
    "```\n",
    "\n",
    "**5. Count:**\n",
    "\n",
    "The `count()` function returns the number of non-NA/null values in the Series.\n",
    "\n",
    "```python\n",
    "count = df['A'].count()\n",
    "```\n",
    "\n",
    "**6. Median:**\n",
    "\n",
    "The `median()` function returns the median of the values for the requested axis.\n",
    "\n",
    "```python\n",
    "median = df['A'].median()\n",
    "```\n",
    "\n",
    "**7. Mode:**\n",
    "\n",
    "The `mode()` function returns the mode(s) of the dataset.\n",
    "\n",
    "```python\n",
    "mode = df['A'].mode()\n",
    "```\n",
    "\n",
    "**8. Standard Deviation:**\n",
    "\n",
    "The `std()` function returns the standard deviation of the values for the requested axis.\n",
    "\n",
    "```python\n",
    "std_dev = df['A'].std()\n",
    "```\n",
    "\n",
    "**9. Variance:**\n",
    "\n",
    "The `var()` function returns the variance of the values for the requested axis.\n",
    "\n",
    "```python\n",
    "variance = df['A'].var()\n",
    "```\n",
    "\n",
    "**10. Aggregate:**\n",
    "\n",
    "The `aggregate()` or `agg()` function is used to apply some aggregation across one or more operations over the specified axis.\n",
    "\n",
    "```python\n",
    "aggregation = df['A'].agg(['sum', 'min', 'max', 'mean', 'median', 'std', 'var'])\n",
    "```\n",
    "\n",
    "Remember to replace `'A'` with the name of your column."
   ],
   "id": "425137177f06a24c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "When calculating the standard deviation of a sample, we use `ddof=1` to perform what is known as Bessel's correction. This correction is necessary because we're trying to estimate the standard deviation of a population based on a sample.\n",
    "\n",
    "In statistics, the standard deviation is calculated slightly differently for populations and samples. For a population, we calculate the standard deviation by taking the square root of the average of the squared deviations from the mean. However, when we're working with a sample, we typically want to estimate the standard deviation of the population, not the sample itself.\n",
    "\n",
    "If we use the same formula for a sample as we do for a population (which is what happens when `ddof=0`), we tend to underestimate the population standard deviation. This is because a sample tends to be less spread out than the population it came from, as it's less likely to include extreme values.\n",
    "\n",
    "Bessel's correction corrects for this by dividing the sum of squared deviations by `n-1` instead of `n` (which is what happens when `ddof=1`). This effectively increases the standard deviation, providing a better estimate of the population standard deviation."
   ],
   "id": "bbfec21c02065fce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Sure, here's an entry on when to use a paired vs unpaired t-test:\n",
    "\n",
    "**Paired t-test:**\n",
    "\n",
    "A paired t-test is used when the observations are not independent of each other. This typically occurs when the observations are taken from the same individual or object at different times or under different conditions. For example, you might use a paired t-test to compare the weights of individuals before and after a diet, or to compare the performance of a machine before and after a maintenance procedure.\n",
    "\n",
    "In Python, you can perform a paired t-test using the `scipy.stats.ttest_rel()` function:\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "# data1 and data2 are arrays of the paired observations\n",
    "t_statistic, p_value = stats.ttest_rel(data1, data2)\n",
    "```\n",
    "\n",
    "**Unpaired t-test:**\n",
    "\n",
    "An unpaired t-test (also known as an independent t-test) is used when the observations are independent of each other. This typically occurs when the observations are taken from two different groups. For example, you might use an unpaired t-test to compare the weights of individuals in two different diet groups, or to compare the performance of two different machines.\n",
    "\n",
    "In Python, you can perform an unpaired t-test using the `scipy.stats.ttest_ind()` function:\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "# data1 and data2 are arrays of the independent observations\n",
    "t_statistic, p_value = stats.ttest_ind(data1, data2)\n",
    "```\n",
    "\n",
    "Remember, the choice between a paired and unpaired t-test depends on the nature of your observations and your experimental design. Always make sure to understand your data and the assumptions of the statistical test before choosing which one to use."
   ],
   "id": "dd6e8275f8492aed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Sure, here's a short cheat sheet on how to find Interquartile Ranges (IQRs) in Python:\n",
    "\n",
    "**Interquartile Range (IQR):**\n",
    "\n",
    "The IQR is a measure of statistical dispersion, being equal to the difference between the upper and lower quartiles. It's often used to find outliers in the data.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Given data\n",
    "data = np.array([1, 2, 2, 2, 3, 1, 1, 15, 2, 2, 2, 3, 1, 1, 2])\n",
    "\n",
    "# Calculate Q1 (25th percentile)\n",
    "Q1 = np.percentile(data, 25)\n",
    "\n",
    "# Calculate Q3 (75th percentile)\n",
    "Q3 = np.percentile(data, 75)\n",
    "\n",
    "# Calculate IQR as difference between Q3 and Q1\n",
    "IQR = Q3 - Q1\n",
    "```\n",
    "\n",
    "In this code, `np.percentile()` is used to find the first quartile (Q1) and the third quartile (Q3). The IQR is then calculated as the difference between Q3 and Q1."
   ],
   "id": "7e03209ecadf716f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Sure, here's a cheat sheet for the requested topics:\n",
    "\n",
    "**1. Imputation Methods:**\n",
    "\n",
    "Imputation is the process of replacing missing data with substituted values. When a value is missing beacuse it was not observed, imputation can be used to fill in the \"gap\" in the data. In Python, the `pandas` library provides the `fillna()` function which can be used for this purpose.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame and 'column' is the column with missing values\n",
    "# Use mean of the column for imputation\n",
    "df['column'].fillna(df['column'].mean(), inplace=True)\n",
    "\n",
    "# Use median of the column for imputation\n",
    "df['column'].fillna(df['column'].median(), inplace=True)\n",
    "\n",
    "# Use mode of the column for imputation\n",
    "df['column'].fillna(df['column'].mode()[0], inplace=True)\n",
    "```\n",
    "\n",
    "**2. Variable Transformations:**\n",
    "\n",
    "Transformations can be used to stabilize variance, make the data more closely align with the normal distribution, improve the interpretability of the data, or satisfy the assumptions of a statistical model. In Python, `numpy` provides functions for common transformations.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Log transformation\n",
    "log_data = np.log(data)\n",
    "\n",
    "# Square root transformation\n",
    "sqrt_data = np.sqrt(data)\n",
    "\n",
    "# Box-Cox transformation\n",
    "from scipy import stats\n",
    "boxcox_data, _ = stats.boxcox(data)\n",
    "```\n",
    "\n",
    "**3. Types of Missingness:**\n",
    "\n",
    "There are three types of missingness: Missing Completely at Random (MCAR), Missing at Random (MAR), and Missing Not at Random (MNAR).\n",
    "\n",
    "- MCAR: The missingness of data is not related to any other variable. This is the ideal scenario but is very rare in practice.\n",
    "- MAR: The missingness of data is related to some other variables but not the variable with missing data.\n",
    "- MNAR: The missingness of data is related to the values of the variable that has missing data.\n",
    "\n",
    "Handling of missing data depends on the type of missingness. MCAR data can be safely omitted. For MAR, you can use methods like multiple imputation or algorithms that support missing values (like XGBoost). For MNAR, it's important to investigate why the data is missing and it may be necessary to make untestable assumptions about the missing data.\n",
    "\n",
    "**4. Handling Outliers:**\n",
    "\n",
    "Outliers are data points that are significantly different from other observations. They can be genuine or due to errors. In Python, the Z-score method or the IQR method can be used to detect and handle outliers.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Z-score method\n",
    "z_scores = stats.zscore(data)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 3)\n",
    "new_data = data[filtered_entries]\n",
    "\n",
    "# IQR method\n",
    "Q1 = np.percentile(data, 25)\n",
    "Q3 = np.percentile(data, 75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "new_data = data[(data > lower_bound) & (data < upper_bound)]\n",
    "```\n",
    "\n",
    "Remember, before removing outliers, it's important to investigate the nature of the outlier. Sometimes, outliers are legitimate data points that indicate important findings or errors in data collection."
   ],
   "id": "a0a7c7d2728bfa81"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Sure, here's how you can calculate these statistics in Python using the pandas and numpy libraries:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your DataFrame and 'column' is the column for which you want to calculate these statistics\n",
    "\n",
    "# Measures of Center\n",
    "mean = df['column'].mean()\n",
    "median = df['column'].median()\n",
    "mode = df['column'].mode()\n",
    "\n",
    "# Measures of Spread\n",
    "range = df['column'].max() - df['column'].min()\n",
    "std_dev = df['column'].std()\n",
    "variance = df['column'].var()\n",
    "\n",
    "# Skewness\n",
    "skewness = df['column'].skew()\n",
    "\n",
    "# Missingness\n",
    "missing_values = df['column'].isnull().sum()\n",
    "total_values = len(df['column'])\n",
    "missingness_percentage = (missing_values / total_values) * 100\n",
    "\n",
    "# Correlation between two variables 'column1' and 'column2'\n",
    "correlation = df['column1'].corr(df['column2'])\n",
    "\n",
    "# Print the calculated statistics\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Median: {median}\")\n",
    "print(f\"Mode: {mode}\")\n",
    "print(f\"Range: {range}\")\n",
    "print(f\"Standard Deviation: {std_dev}\")\n",
    "print(f\"Variance: {variance}\")\n",
    "print(f\"Skewness: {skewness}\")\n",
    "print(f\"Missingness: {missingness_percentage}%\")\n",
    "print(f\"Correlation between column1 and column2: {correlation}\")\n",
    "```\n",
    "\n",
    "In this code:\n",
    "\n",
    "- The mean, median, and mode are measures of center. They give you a sense of the \"typical\" value of the variable.\n",
    "- The range, standard deviation, and variance are measures of spread. They give you a sense of how much the values of the variable vary.\n",
    "- Skewness is a measure of the asymmetry of the probability distribution.\n",
    "- Missingness is the percentage of missing values in the variable. High missingness can bias the results of your analysis.\n",
    "- The correlation is a measure of how two variables move in relation to each other."
   ],
   "id": "14a254f239267bf8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "There are several common methods for handling missing data in statistical analysis:\n",
    "\n",
    "1. **Listwise Deletion:** Also known as complete case analysis, the simplest method is to remove entire observations that have missing values. However, this method can only be used when the data is missing completely at random (MCAR).\n",
    "\n",
    "```python\n",
    "df.dropna(inplace=True)\n",
    "```\n",
    "\n",
    "2. **Pairwise Deletion:** This method, also known as available case analysis, uses all the available data to compute the statistical analysis. For example, if you are correlating multiple variables, it will use every case where each pair of variables is available.\n",
    "\n",
    "3. **Mean/Median/Mode Imputation:** Replace the missing values with the mean, median, or mode of that variable. This is a quick and easy method, but it can distort the distribution of the data.\n",
    "\n",
    "```python\n",
    "# Mean imputation\n",
    "df['column'].fillna(df['column'].mean(), inplace=True)\n",
    "\n",
    "# Median imputation\n",
    "df['column'].fillna(df['column'].median(), inplace=True)\n",
    "\n",
    "# Mode imputation\n",
    "df['column'].fillna(df['column'].mode()[0], inplace=True)\n",
    "```\n",
    "\n",
    "4. **Last Observation Carried Forward (LOCF) & Next Observation Carried Backward (NOCB):** These are methods used in time series data where missing values are replaced with either the next valid observation or the last valid observation.\n",
    "\n",
    "5. **Interpolation:** In time series data, missing values can be replaced by interpolating between valid observations. The 'interpolate()' function in pandas uses various methods like linear, time, and polynomial to interpolate missing values.\n",
    "\n",
    "```python\n",
    "df.interpolate(method ='linear', limit_direction ='forward')\n",
    "```\n",
    "\n",
    "6. **Multiple Imputation:** Multiple imputation is a more sophisticated method that fills missing values multiple times to create \"complete\" datasets. Analysis is then performed on all datasets and results are pooled. This can be done using the 'mice' package in R or the 'IterativeImputer' class in sklearn in Python.\n",
    "\n",
    "```python\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "imputer = IterativeImputer()\n",
    "df_imputed = imputer.fit_transform(df)\n",
    "```\n",
    "\n",
    "Remember, the method you choose depends on the nature of your data and the reason why the data is missing. Always try to understand why data is missing before choosing a method to handle it."
   ],
   "id": "fa2bee053f454257"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
