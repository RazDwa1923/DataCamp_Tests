{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "Sure, here's an example of using the Bagging method with a Decision Tree classifier in Python using the sklearn library:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a binary classification dataset.\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# Create a Bagging classifier with the Decision Tree as the base estimator\n",
    "bagging = BaggingClassifier(base_estimator=tree, n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the Bagging classifier\n",
    "bagging.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bagging.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the Bagging classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "```\n",
    "\n",
    "In this script, we first generate a binary classification dataset using the `make_classification` function from sklearn. We then split this dataset into a training set and a test set.\n",
    "\n",
    "We create a Decision Tree classifier and a Bagging classifier. The Bagging classifier uses the Decision Tree as its base estimator and creates 100 such estimators.\n",
    "\n",
    "We train the Bagging classifier on the training data and make predictions on the test data. Finally, we calculate the accuracy of the Bagging classifier by comparing the predicted labels to the true labels."
   ],
   "id": "5db5eb0a6ffe0223"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Create a PCA model with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the PCA model to the data\n",
    "pca.fit(X)\n",
    "\n",
    "# Transform the data to the first 2 principal components\n",
    "X_pca = pca.transform(X)\n",
    "\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)"
   ],
   "id": "ec36e4e6f64ea74e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "eac1db43984c465b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "4501bcf7719681ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Create some data\n",
    "data = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "\n",
    "# Create a PowerTransformer\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "# Fit the PowerTransformer to the data\n",
    "pt.fit(data)\n",
    "\n",
    "# Transform the data\n",
    "data_log_transformed = pt.transform(data)\n",
    "\n",
    "print(data_log_transformed)"
   ],
   "id": "8072f4d11535e99d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Generate a binary classification dataset.\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, random_state=42)\n",
    "\n",
    "# Split the dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a classifier\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "y_score = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute ROC AUC\n",
    "roc_auc = roc_auc_score(y_test, y_score)\n",
    "\n",
    "print(f'ROC AUC: {roc_auc}')"
   ],
   "id": "a2559b45549fbad5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# Train the Decision Tree classifier\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the Decision Tree classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')"
   ],
   "id": "4cd00761237e8068"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Assuming 'X' is your data\n",
    "# X = ...  # replace with your actual data\n",
    "\n",
    "# Create a KMeans instance with 3 clusters (you can change this value)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "\n",
    "# Fit the model to your data\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Predict the clusters for your data\n",
    "labels = kmeans.predict(X)\n",
    "\n",
    "# Print the cluster centers\n",
    "print(\"Cluster centers:\")\n",
    "print(kmeans.cluster_centers_)\n",
    "\n",
    "# Print the labels for your data\n",
    "print(\"Labels:\")\n",
    "print(labels)"
   ],
   "id": "c8eba64669f91d29"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c3e132ee3981a77e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Assuming you have the following variables:\n",
    "# y_true: true labels\n",
    "# y_pred: predicted labels\n",
    "# y_score: predicted scores\n",
    "\n",
    "# Classification Metrics\n",
    "# ----------------------\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_true, y_pred)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_true, y_pred)\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# ROC AUC Score\n",
    "roc_auc = roc_auc_score(y_true, y_score)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(y_true, y_pred)\n",
    "\n",
    "# Regression Metrics\n",
    "# ------------------\n",
    "# Mean Squared Error\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "\n",
    "# Mean Absolute Error\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "# R2 Score\n",
    "r2 = r2_score(y_true, y_pred)"
   ],
   "id": "3ca89e7e4acdf6ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from scipy.cluster.hierarchy import linkage, cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'X' is your data\n",
    "X = np.random.rand(100, 2)  # replace with your actual data\n",
    "\n",
    "# Compute the linkage with 'complete' method and 'euclidean' distance\n",
    "Z = linkage(X, method='complete', metric='euclidean')\n",
    "\n",
    "# Compute the cluster distances\n",
    "c, coph_dists = cophenet(Z, pdist(X))\n",
    "\n",
    "print(f'Cophenetic Correlation Coefficient: {c}')"
   ],
   "id": "4a7ba154d16c49fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a binary classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=2, n_redundant=10, random_state=42)\n",
    "\n",
    "# Split the dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gradient Boosting classifier\n",
    "gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=42)\n",
    "\n",
    "# Train the Gradient Boosting classifier\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gbc.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the Gradient Boosting classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Gradient Boosting Classifier Accuracy: {accuracy*100:.2f}%')"
   ],
   "id": "783c7822467f9547"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate a regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=2, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gradient Boosting regressor\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=42, loss='ls')\n",
    "\n",
    "# Train the Gradient Boosting regressor\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gbr.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error of the Gradient Boosting regressor\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f'Gradient Boosting Regressor Mean Squared Error: {mse:.2f}')"
   ],
   "id": "c1d39e6813121262"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=2, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Linear Regression model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Print the coefficients of the Linear Regression model\n",
    "print(f'Coefficients: {lr.coef_}')\n",
    "\n",
    "# Print the intercept of the Linear Regression model\n",
    "print(f'Intercept: {lr.intercept_}')"
   ],
   "id": "eb52935cffcdc2eb"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
